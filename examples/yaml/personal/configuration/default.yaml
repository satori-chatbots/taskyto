
#llm_services:
#  - name: llama2chat_on_llm_server
#    extension: 'llm_server/llm.py'
#    args:
#      model: "meta-llama/Llama-2-7b-chat-hf"

llm_services:
  - name: ollama-gemma
    extension: 'ollama'
    args:
      model: "gemma3:4b"
      host: "${OLLAMA_HOST}"

default_llm:
  id: ollama-gemma
  temperature: 0.0
